{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08d215dc",
   "metadata": {},
   "source": [
    "# Notebook: Load, Clean, and Analyze Campus Incident Data\n",
    "This notebook performs data loading and cleaning (Notebook 2) and analysis + visualization (Notebook 3) for the campus incidents dataset. It also displays the corrected Jewish-holiday metrics and matched raw rows produced by the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4248cb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: imports and helper functions\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Nice defaults\n",
    "sns.set(style='whitegrid', context='notebook')\n",
    "pd.set_option('display.max_columns', 200)\n",
    "\n",
    "# Paths\n",
    "ROOT = Path('..') if Path('.').resolve().name != 'AntisemitismTrends' else Path('.')\n",
    "DATA_DIR = Path('../data').resolve() if (Path('../data').exists()) else Path('data')\n",
    "OUT_DIR = Path('../outputs').resolve() if (Path('../outputs').exists()) else Path('outputs')\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "RAW_CSV = DATA_DIR / 'campus_reports.csv'\n",
    "CLEAN_PARQUET = OUT_DIR / 'campus_reports.cleaned.parquet'\n",
    "METRICS_CSV = OUT_DIR / 'protest_jewish_holiday_metrics_corrected.csv'\n",
    "MATCHED_RAW = OUT_DIR / 'inspection_jh_raw_matches.csv'\n",
    "PR_PLOT = OUT_DIR / 'pr_curve_best_holiday_model.png'\n",
    "RATE_PLOT = OUT_DIR / 'protest_rate_by_jewish_holiday_corrected.png'\n",
    "\n",
    "print('Paths set:', RAW_CSV, CLEAN_PARQUET, METRICS_CSV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba10fc5b",
   "metadata": {},
   "source": [
    "## Notebook 2 — Load CSV and quick inspection\n",
    "Read the CSV with safe options, detect malformed lines (comment prefix `//` accepted), and show initial diagnostics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fae87c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 Load with protective options\n",
    "read_opts = dict(\n",
    "    sep=',',\n",
    "    quotechar='\"',\n",
    "    encoding='utf-8',\n",
    "    dtype=str,\n",
    "    keep_default_na=True,\n",
    "    na_values=['', 'NA', 'N/A'],\n",
    "    engine='python'  # more tolerant for tricky rows\n",
    ")\n",
    "\n",
    "# Some files have '//' comment sections in exported excerpts; support comment prefix\n",
    "with open(RAW_CSV, 'r', encoding='utf-8') as f:\n",
    "    sample = ''.join([next(f) for _ in range(10)])\n",
    "print('Sample header lines:\\n', sample)\n",
    "\n",
    "raw = pd.read_csv(RAW_CSV, **read_opts, comment='//')\n",
    "print('Loaded raw shape:', raw.shape)\n",
    "print(raw.dtypes)\n",
    "raw.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88e3d3f",
   "metadata": {},
   "source": [
    "## Notebook 2 — Parse dates and enforce dtypes\n",
    "Parse `Date of Incident` to datetime, show parsing failures, and cast categorical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf89dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 Parse dates\n",
    "date_col = 'Date of Incident'\n",
    "raw[date_col+'_parsed'] = pd.to_datetime(raw[date_col], format='%m/%d/%y', errors='coerce')\n",
    "# fallback: try infer for the rows that failed\n",
    "mask_fail = raw[date_col+'_parsed'].isna()\n",
    "if mask_fail.any():\n",
    "    raw.loc[mask_fail, date_col+'_parsed'] = pd.to_datetime(raw.loc[mask_fail, date_col], errors='coerce', dayfirst=False)\n",
    "\n",
    "print('Parsed datetimes: total failures =', raw[date_col+'_parsed'].isna().sum())\n",
    "raw.loc[raw[date_col+'_parsed'].isna(), [date_col]].head(10)\n",
    "\n",
    "# Enforce categorical dtypes\n",
    "cat_cols = ['State of Incident', 'Incident Type', 'College/University']\n",
    "for c in cat_cols:\n",
    "    if c in raw.columns:\n",
    "        raw[c] = raw[c].astype('category')\n",
    "\n",
    "raw[date_col+'_parsed'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26498501",
   "metadata": {},
   "source": [
    "## Notebook 2 — Clean Description text (quotes, escape sequences, PII)\n",
    "Normalize the `Description` field, mask phone numbers and emails, compute description length and flags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e654b956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3 Clean Description\n",
    "desc_col = 'Description'\n",
    "if desc_col in raw.columns:\n",
    "    def mask_pii(text):\n",
    "        if pd.isna(text):\n",
    "            return text\n",
    "        t = str(text)\n",
    "        # unescape doubled quotes\n",
    "        t = t.replace('\"\"', '\"')\n",
    "        # mask phone numbers\n",
    "        t = re.sub(r'\\b\\d{3}[-.\\s]?\\d{3}[-.\\s]?\\d{4}\\b', '[PHONE]', t)\n",
    "        # mask emails\n",
    "        t = re.sub(r'[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}', '[EMAIL]', t)\n",
    "        return t.strip()\n",
    "\n",
    "    raw['description_clean'] = raw[desc_col].apply(mask_pii)\n",
    "    raw['description_len'] = raw['description_clean'].fillna('').str.len()\n",
    "    raw['description_empty'] = raw['description_len'] == 0\n",
    "    display(raw[[desc_col, 'description_clean', 'description_len', 'description_empty']].head(5))\n",
    "else:\n",
    "    print('No Description column found')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c444f6ab",
   "metadata": {},
   "source": [
    "## Notebook 2 — Normalize College/University names and locations\n",
    "Apply a small canonical mapping; fallback to original. Validate states against the USPS state list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05b49e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.4 Normalize College names (small mapping example)\n",
    "from hashlib import md5\n",
    "\n",
    "college_col = 'College/University'\n",
    "if college_col in raw.columns:\n",
    "    mapping = {\n",
    "        'California State University, Northridge': 'CSUN',\n",
    "        'San Francisco State University': 'San Francisco State',\n",
    "        # Add more mappings as needed\n",
    "    }\n",
    "    raw['college_canonical'] = raw[college_col].map(mapping).fillna(raw[college_col])\n",
    "    # create campus_id as hash of canonical name + city\n",
    "    raw['campus_id'] = raw[['college_canonical', 'City of Incident']].fillna('').apply(lambda row: md5((str(row['college_canonical']) + '|' + str(row['City of Incident'])).encode('utf-8')).hexdigest(), axis=1)\n",
    "    raw[['College/University', 'college_canonical', 'campus_id']].head()\n",
    "else:\n",
    "    print('No College/University column found')\n",
    "\n",
    "# Validate State of Incident against USPS two-letter list\n",
    "usps = set(['Alabama','Alaska','Arizona','Arkansas','California','Colorado','Connecticut','Delaware','Florida','Georgia','Hawaii','Idaho','Illinois','Indiana','Iowa','Kansas','Kentucky','Louisiana','Maine','Maryland','Massachusetts','Michigan','Minnesota','Mississippi','Missouri','Montana','Nebraska','Nevada','New Hampshire','New Jersey','New Mexico','New York','North Carolina','North Dakota','Ohio','Oklahoma','Oregon','Pennsylvania','Rhode Island','South Carolina','South Dakota','Tennessee','Texas','Utah','Vermont','Virginia','Washington','West Virginia','Wisconsin','Wyoming','Washington D.C.'])\n",
    "if 'State of Incident' in raw.columns:\n",
    "    raw['state_valid'] = raw['State of Incident'].isin(usps)\n",
    "    print('Invalid state count:', (~raw['state_valid']).sum())\n",
    "    raw.loc[~raw['state_valid'], 'State of Incident'].value_counts().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a4fe6b",
   "metadata": {},
   "source": [
    "## Notebook 2 — Handle missing values and duplicates\n",
    "Identify missing values and deduplicate rows; add `is_duplicate` flag and drop duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e60b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.5 Missing values summary\n",
    "missing = raw.isna().sum().sort_values(ascending=False)\n",
    "print('Missing per column:\\n', missing[missing>0].head(20))\n",
    "\n",
    "# De-duplicate: consider date + campus_id + incident type + description hash\n",
    "raw['desc_hash'] = raw['description_clean'].fillna('').apply(lambda x: md5(x.encode('utf-8')).hexdigest())\n",
    "subset = [date_col+'_parsed', 'campus_id', 'Incident Type', 'desc_hash']\n",
    "raw['is_duplicate'] = raw.duplicated(subset=subset, keep='first')\n",
    "print('Duplicates found:', raw['is_duplicate'].sum())\n",
    "\n",
    "# drop duplicates for cleaned dataset\n",
    "clean = raw[~raw['is_duplicate']].copy()\n",
    "print('Cleaned shape after dropping duplicates:', clean.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceeda1a4",
   "metadata": {},
   "source": [
    "## Notebook 2 — Validate and save cleaned CSV / parquet\n",
    "Run schema checks and save cleaned data to compressed parquet and CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e95dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.6 Basic schema checks\n",
    "required = [date_col+'_parsed', 'campus_id', 'Incident Type']\n",
    "for r in required:\n",
    "    if r not in clean.columns:\n",
    "        raise RuntimeError(f'Missing required column: {r}')\n",
    "    if clean[r].isna().any():\n",
    "        print(f'Warning: nulls in required column {r}:', clean[r].isna().sum())\n",
    "\n",
    "# Save: try parquet, fall back to CSV if parquet engine missing\n",
    "csv_path = OUT_DIR / 'campus_reports.cleaned.csv'\n",
    "try:\n",
    "    import pyarrow  # type: ignore\n",
    "    clean.to_parquet(CLEAN_PARQUET, index=False, compression='brotli')\n",
    "    print('Saved cleaned data to', CLEAN_PARQUET)\n",
    "    # quick reload assert\n",
    "    _back = pd.read_parquet(CLEAN_PARQUET)\n",
    "    assert len(_back) == len(clean)\n",
    "    print('Roundtrip rows OK (parquet)')\n",
    "except Exception as e:\n",
    "    # fallback\n",
    "    clean.to_csv(csv_path, index=False)\n",
    "    print('Parquet unavailable; saved cleaned CSV to', csv_path)\n",
    "    _back = pd.read_csv(csv_path)\n",
    "    assert len(_back) == len(clean)\n",
    "    print('Roundtrip rows OK (csv)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23469301",
   "metadata": {},
   "source": [
    "---\n",
    "# Notebook 3 — Analysis & Visualization\n",
    "Load cleaned data and prepare aggregated tables, time-series, maps, and NLP summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9707825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Load cleaned data\n",
    "try:\n",
    "    clean = pd.read_parquet(CLEAN_PARQUET)\n",
    "    print('Loaded cleaned shape (parquet):', clean.shape)\n",
    "except Exception:\n",
    "    csv_path = OUT_DIR / 'campus_reports.cleaned.csv'\n",
    "    if csv_path.exists():\n",
    "        clean = pd.read_csv(csv_path)\n",
    "        print('Loaded cleaned shape (csv):', clean.shape)\n",
    "    else:\n",
    "        raise RuntimeError('Cleaned data not found: tried parquet and csv')\n",
    "\n",
    "clean.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93343c8c",
   "metadata": {},
   "source": [
    "## Notebook 3 — Aggregate incidents by date, state, and campus\n",
    "Compute daily, weekly, and monthly counts; save aggregated DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4be2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 Aggregations\n",
    "# Ensure the parsed date column is a datetime64 dtype so pd.Grouper works\n",
    "clean[date_col+'_parsed'] = pd.to_datetime(clean[date_col+'_parsed'], errors='coerce')\n",
    "# keep a simple python date column for daily counts/display\n",
    "clean['date'] = clean[date_col+'_parsed'].dt.date\n",
    "\n",
    "daily = clean.groupby('date').size().rename('incidents').reset_index()\n",
    "# weekly/monthly using the datetime column (requires datetime64 dtype)\n",
    "weekly = clean.groupby(pd.Grouper(key=date_col+'_parsed', freq='W'))['campus_id'].count().rename('incidents').reset_index()\n",
    "monthly = clean.groupby(pd.Grouper(key=date_col+'_parsed', freq='M'))['campus_id'].count().rename('incidents').reset_index()\n",
    "\n",
    "daily.head(), weekly.head(), monthly.head()\n",
    "\n",
    "# save\n",
    "daily.to_csv(OUT_DIR / 'agg_daily.csv', index=False)\n",
    "weekly.to_csv(OUT_DIR / 'agg_weekly.csv', index=False)\n",
    "monthly.to_csv(OUT_DIR / 'agg_monthly.csv', index=False)\n",
    "print('Saved aggregated tables')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cf560e",
   "metadata": {},
   "source": [
    "## Notebook 3 — Time-series plots and rolling averages\n",
    "Plot daily counts with 7-day and 30-day rolling means, annotate notable dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1a1673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3 Time-series\n",
    "daily['date'] = pd.to_datetime(daily['date'])\n",
    "daily = daily.sort_values('date')\n",
    "daily['rolling7'] = daily['incidents'].rolling(7, center=True, min_periods=1).mean()\n",
    "daily['rolling30'] = daily['incidents'].rolling(30, center=True, min_periods=1).mean()\n",
    "\n",
    "plt.figure(figsize=(14,5))\n",
    "plt.plot(daily['date'], daily['incidents'], alpha=0.5, label='Daily')\n",
    "plt.plot(daily['date'], daily['rolling7'], label='7-day MA')\n",
    "plt.plot(daily['date'], daily['rolling30'], label='30-day MA')\n",
    "plt.legend()\n",
    "plt.title('Campus incidents: daily counts and rolling means')\n",
    "plt.tight_layout()\n",
    "plot_path = OUT_DIR / 'daily_time_series.png'\n",
    "plt.savefig(plot_path)\n",
    "print('Saved', plot_path)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78448206",
   "metadata": {},
   "source": [
    "## Notebook 3 — Incident-type breakdowns and top campuses\n",
    "Show counts by incident type and top campuses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8c406a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.4 Incident type breakdown\n",
    "types = clean['Incident Type'].value_counts().reset_index()\n",
    "types.columns = ['Incident Type', 'count']\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.barplot(data=types, x='count', y='Incident Type')\n",
    "plt.title('Incidents by type')\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUT_DIR / 'incidents_by_type.png')\n",
    "print('Saved incidents_by_type.png')\n",
    "plt.show()\n",
    "\n",
    "# Top campuses\n",
    "top_campuses = clean['college_canonical'].value_counts().head(20).reset_index()\n",
    "top_campuses.columns = ['college', 'count']\n",
    "top_campuses.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3b9396",
   "metadata": {},
   "source": [
    "## Notebook 3 — Basic NLP on Description\n",
    "Compute top tokens and bigrams (basic). Wordcloud optional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5ca215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.5 Basic NLP (token counts)\n",
    "from collections import Counter\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "text_series = clean['description_clean'].dropna().astype(str)\n",
    "all_text = ' '.join(text_series)\n",
    "# simple tokenization\n",
    "tokens = [t.lower() for t in re.findall(r\"\\b\\w{3,}\\b\", all_text)]\n",
    "filtered = [t for t in tokens if t not in stop]\n",
    "most = Counter(filtered).most_common(30)\n",
    "most[:20]\n",
    "\n",
    "# Save top tokens\n",
    "pd.DataFrame(most, columns=['token','count']).to_csv(OUT_DIR / 'top_tokens.csv', index=False)\n",
    "print('Saved top_tokens.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903bf771",
   "metadata": {},
   "source": [
    "## Notebook 3 — Export figures and summary tables\n",
    "List saved artifacts and show corrected Jewish-holiday metrics + matched raw rows (if present)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba2f2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.6 Show saved artifacts and the corrected metrics + matched rows\n",
    "artifacts = sorted([str(p) for p in OUT_DIR.glob('*')])\n",
    "print('Artifacts in outputs:')\n",
    "for a in artifacts:\n",
    "    print(' -', a)\n",
    "\n",
    "# Display corrected metrics CSV if present\n",
    "if METRICS_CSV.exists():\n",
    "    print('\\nCorrected Jewish-holiday metrics:')\n",
    "    display(pd.read_csv(METRICS_CSV))\n",
    "else:\n",
    "    print('\\nNo corrected metrics CSV found at', METRICS_CSV)\n",
    "\n",
    "# Display matched raw rows\n",
    "if MATCHED_RAW.exists():\n",
    "    print('\\nMatched raw rows (first 30):')\n",
    "    matched = pd.read_csv(MATCHED_RAW)\n",
    "    display(matched.head(30))\n",
    "else:\n",
    "    print('\\nNo matched raw rows file found at', MATCHED_RAW)\n",
    "\n",
    "# Display relevant plots if present\n",
    "from IPython.display import Image, display\n",
    "for p in [RATE_PLOT, PR_PLOT, OUT_DIR / 'daily_time_series.png']:\n",
    "    if Path(p).exists():\n",
    "        print('Showing:', p)\n",
    "        display(Image(str(p)))\n",
    "    else:\n",
    "        print('Missing plot:', p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44a65b2",
   "metadata": {},
   "source": [
    "## Short Summary — Jewish-holiday vs protest check (from pipeline)\n",
    "- Contingency and Fisher exact test were computed in the pipeline and saved to `outputs/protest_jewish_holiday_metrics_corrected.csv`.\n",
    "- The pipeline-run result (using the user-provided canonical holiday ranges) yielded roughly similar protest rates on Jewish holidays vs non-holidays; see the metrics table above for exact counts and p-value."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
